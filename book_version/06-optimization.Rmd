---
title: "Numerical Optimization"
author: Zhentao Shi
date: "Jan 30, 2019"
bibliography: book.bib
biblio-style: apalike
link-citations: yes
fontsize: 11pt
urlcolor: blue
output:
  pdf_document: default
---

# Numerical Optimization

Optimization is a key step to implement extremum estimators in econometrics.
A general optimization problem is formulated as
$$\min_{\theta \in \Theta } f(\theta) \,\, \mbox{ s.t. }  g(\theta) = 0, h(\theta) \leq 0,$$
where $f(\cdot)$ is a criterion function, $g(\theta) = 0$ is an equality constraint,
and $h(\theta)\leq 0$ is an inequality constraint.

Most established numerical optimization algorithms aim at finding a local minimum.
However, there is no guarantee to locate the global minimum when multiple local minima exist.

Optimization without the equality and/or inequality constraints is called
an *unconstrained* problem; otherwise it is called a *constrained* problem.
The constraints can be incorporated into the criterion function via Lagrangian.

## Methods

There are many optimization algorithms in the field of operational research;
they are variants of a small handful of fundamental principles.

The essential idea for optimizing a twice-differentiable objective function is the Newton's method.
A necessary condition for optimization is the first-order condition
$s(\theta) = \partial f(\theta) / \partial \theta = 0$.

At an initial trial value $\theta_0$, if $s(\theta_0) \neq 0$, the search is updated by
$$
\theta_{t+1} = \theta_{t} -  \left( H(\theta_t)  \right)^{-1}  s(\theta_t)
$$
for $t=0,1,\cdots$ where
$H(\theta) = \frac{ \partial s(\theta )}{ \partial \theta}$
is the Hessian matrix.
The algorithm iterates
until $|\theta_{t+1} -\theta_{t}| < \epsilon$ (absolute criterion) and/or
$|\theta_{t+1} -\theta_{t}|/|\theta_{t}| < \epsilon$ (relative criterion), where
$\epsilon$ is a small positive number chosen as a tolerance level.



**Newton's Method.** Newton's method seeks the solution to
$s(\theta) = 0$. Recall that the first-order condition is a necessary condition but not a sufficient
condition. We still need to verify the second-order condition to verify
whether a root to $s(\theta)$ is associated with a minimizer, a maximizer or a saddle point.
If there are multiple minima, we compare the value at each to decide the
global minimum.

It is clear that Newton's method requires
computation of the gradient $s(\theta)$ and the Hessian $H(\theta)$.
Newton's method converges at quadratic rate, which is fast.

**Quasi-Newton Method.** The most well-known quasi-Newton algorithm is  
[BFGS](http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm).
It avoids explicit calculation of the computationally expensive Hessian matrix. Instead, starting from an initial (inverse)
Hessian, it updates the Hessian by an explicit formula motivated from the idea of quadratic approximation.

**Derivative-Free Method.** [Nelder-Mead](http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)
is a simplex method. It searches a local minimum by reflection, expansion and contraction.

## Implementation

R's optimization infrastructure has been constantly improving.
[R Optimization Task View](http://cran.r-project.org/web/views/Optimization.html)
gives a survey of the available CRAN packages.

For general-purpose nonlinear optimization, the package
[`optimx`](http://cran.r-project.org/web/packages/optimx/index.html) [@nash2014best]
effectively replaces R's default optimization commands. `optimx` provides a unified
interface for various widely-used optimization algorithms. Moreover,
it facilitates comparison among optimization algorithms.

**Example**

We use `optimx` to solve pseudo Poisson maximum likelihood estimation (PPML).
If $y_i$ is a continuous random variable, it obviously does not follow a Poisson
distribution, whose support consists of non-negative integers. However, if the conditional mean model
$$E[y_i | x_i] = \exp( x_i' \beta),$$
is satisfied, we can still use the Poisson regression to obtain a consistent
estimator of the parameter $\beta$ even if $y_i$ does not follow a conditional
Poisson distribution.

If $Z$ follows a Poisson distribution with mean $\lambda$, the probability mass function
$$
\Pr(Z = k) = \frac{\mathrm{e}^{-\lambda} \lambda^k}{k!}, \mbox{ for }k=0,1,2,\ldots,
$$
so that
$$
\log \Pr(Y = y | x) =  -\exp(x'\beta) + y\cdot x'\beta - \log k!
$$
Since the last term is irrelevant to the parameter, the
log-likelihood function is
$$
\ell(\beta) = \log \Pr( \mathbf{y} | \mathbf{x};\beta ) =
-\sum_{i=1}^n \exp(x_i'\beta) + \sum_{i=1}^n y_i x_i'\beta.
$$
In addition, it is easy to write the gradient
$$
s(\beta) =\frac{\partial \ell(\beta)}{\partial \beta} =
-\sum_{i=1}^n \exp(x_i'\beta)x_i + \sum_{i=1}^n y_i x_i.
$$
and verify that the Hessian
$$
H(\beta) = \frac{\partial^2 \ell(\beta)}{\partial \beta \partial \beta'} =
-\sum_{i=1}^n \exp(x_i'\beta)x_i x_i'
$$
is negative definite. Therefore, $\ell(\beta)$ is strictly concave
in $\beta$.

In operational reserach, the default optimization is minimization, although
economics has utility maximization and cost minimization and
statistics has maximum likelihood estimation and minimal least squared estimation.
To follow this convention in operational research, here we formulate the *negative* log-likelihood.

```{r,cache=TRUE}
# Poisson likelihood
poisson.loglik = function( b ) {
  b = as.matrix( b )
  lambda =  exp( X %*% b )
  ell = -sum( -lambda + y *  log(lambda) )
  return(ell)
}
```

To implement optimization in `R`, it is recommended to write the criterion as a
function of the parameter. Data can be fed inside or outside of the function.
If the data is provided as additional arguments, these arguments must be explicit.
(In constrast, in `Matlab` the parameter must be the sole argument for the function to be
optimized, and data can only be injected through a nested function.)

```{r,cache=TRUE,tidy=TRUE,message=FALSE,warning=FALSE}
#implement both BFGS and Nelder-Mead for comparison.

library(AER)
library(numDeriv)
library(optimx)

## prepare the data
data("RecreationDemand")
y =  RecreationDemand$trips
X =  with(RecreationDemand, cbind(1, income) )

## estimation
b.init =  c(0,1)  # initial value
b.hat = optimx( b.init, poisson.loglik,
                method = c("BFGS", "Nelder-Mead"),
                 control = list(reltol = 1e-7,
                                abstol = 1e-7)  )
print( b.hat )
```

Given the conditional mean model, nonlinear least squares (NLS) is also consistent in theory.
NLS minimizes
$$
\sum_{i=1}^n (y_i - \exp(x_i \beta))^2
$$
A natural question is: why do we prefer PPML to NLS?
My argument is that, PPML's optimization for the linear index is globally convex, while NLS is not.
It implies that the numerical optimization of PPML is easier and more robust than that of NLS. I leave the derivation of the non-convexity of NLS
as an exercise.


In practice no algorithm suits all problems. Simulation, where the true parameter is known,
  is helpful to check the accuracy of one's optimization routine before applying to an empirical problem,
  where the true parameter is unknown.
Contour plot is a useful tool to visualize the function surface/manifold in a low dimension.

**Example**

```{r,cache=TRUE,tidy=TRUE}
x.grid = seq(0, 1.8, 0.02)
x.length = length(x.grid)
y.grid = seq(-.5, .2, 0.01)
y.length = length(y.grid)

z.contour = matrix(0, nrow = x.length, ncol = y.length)

for (i in 1:x.length){
  for (j in 1:y.length){
    z.contour[i,j] = poisson.loglik( c( x.grid[i], y.grid[j] )  )
  }
}

contour( x.grid,  y.grid, z.contour, 20)
```

For problems that demand more accuracy,  third-party standalone solvers can be
invoked via interfaces to `R`.
For example, we can access [`NLopt`](http://ab-initio.mit.edu/wiki/index.php/NLopt_Installation)
through the packages [`nloptr`](http://cran.r-project.org/web/packages/nloptr/index.html).
However, standalone solvers usually have to be compiled and configured.
These steps are often not as straightforward as installing commercial Windows software.

`NLopt` offers an [extensive list of algorithms](http://ab-initio.mit.edu/wiki/index.php/NLopt_Algorithms#SLSQP).

**Example**

We first carry out the Nelder-Mead algorithm in NLOPT.

```{r,cache=TRUE,tidy=TRUE}
library(nloptr)
## optimization with NLoptr

opts = list("algorithm"="NLOPT_LN_NELDERMEAD",
            "xtol_rel"=1.0e-7,
            maxeval = 500
)

res_NM = nloptr( x0=b.init,
                 eval_f=poisson.loglik,
                 opts=opts)
print( res_NM )

# "SLSQP" is indeed the BFGS algorithm in NLopt,
# though "BFGS" doesn't appear in the name
opts = list("algorithm"="NLOPT_LD_SLSQP","xtol_rel"=1.0e-7)
```


To invoke BFGS in NLOPT, we must code up the gradient $s(\beta)$,
as in the function `poisson.log.grad()` below.


```{r,cache=TRUE,tidy=TRUE}
poisson.loglik.grad = function( b ) {
  b = as.matrix( b )
  lambda =  exp( X %*% b )
  ell = -colSums( -as.vector(lambda) * X + y *  X )
  return(ell)
}
```
We compare the analytical gradient with the numerical gradient to make sure
the function is correct.

```{r,cache=TRUE,tidy=TRUE}
# check the numerical gradient and the analytical gradient
b = c(0,.5)
grad(poisson.loglik, b)
poisson.loglik.grad(b)
```

With the function of gradient, we are ready for BFGS.

```{r,cache=TRUE,tidy=TRUE}
res_BFGS = nloptr( x0=b.init,
                   eval_f=poisson.loglik,
                   eval_grad_f = poisson.loglik.grad,
                   opts=opts)
print( res_BFGS )
```


## Contrained Optimization

* `optimx` can handle simple box-constrained problems.
* Some algorithms in `nloptr`, for example, `NLOPT_LD_SLSQP`, can handle nonlinear constrained problems.


* New packages to be elaborated next year:
[ROI](https://cran.r-project.org/web/packages/ROI/index.html) General R optimization infrastructure


## Convex Optimization

If a function is convex in its argument, then a local minimum is a global minimum.
Convex optimization is particularly important in high-dimensional problems. The readers are
referred to @boyd2004convex for an accessible comprehensive treatment. They claim that
"convex optimization is technology; all other optimizations are arts." This is true to some extent.


**Example**

* linear regression model MLE


Normal MLE. The (negative) log-likelihood is
$$
\ell (\beta, \sigma) = \log \sigma + \frac{1}{\sigma^2}\sum_{i=1}^n (y_i - x_i' \beta)^2
$$
This is not a convex problem, because $\log \sigma$ is concave. But if we reparameterize the criterion function by $\gamma = 1/\sigma$ and $\alpha = \beta / \sigma$, then
$$
\ell (\alpha, \gamma) = -\log \gamma + \sum_{i=1}^n (\gamma y_i - x_i' \alpha)^2
$$
This function in convex in $\alpha, \gamma$.

Indeed, many MLE estimators in the first-year econometrics are convex.



* Lasso [@su2016identifying]
* Relaxed empirical likelihood [@shi2016econometric].
* A companion paper can be found here [@gao2018two].

A class of common convex optimization can be reliably implemented in `R`.
[`Rmosek`](http://rmosek.r-forge.r-project.org/) is an interface in `R` to access `Mosek`.
`Mosek` is a high-quality commercial solver dedicated to convex optimization.
It offers free academic licenses. (`Rtools` is a prerequisite to install `Rmosek`.)

```{r,eval=FALSE,tidy=TRUE}
lo1 <- list()
lo1$sense <- "max"
lo1$c <- c(3,1,5,1)
lo1$A <- Matrix::Matrix(c(3,1,2,0,
                  2,1,3,1,
                  0,2,0,3),
          nrow=3, byrow=TRUE, sparse=TRUE)
lo1$bc <- rbind(blc = c(30,15,-Inf),
                buc = c(30,Inf,25))
lo1$bx <- rbind(blx = c(0,0,0,0),
                bux = c(Inf,10,Inf,Inf))
r <- Rmosek::mosek(lo1) # as today (Feb 10, 2019), the latest version of
# Rmosek is not functional. I can make it work on R3.4, but not the latest R3.5
```




* Convex solvers: CLEPX, MOSEK, Gurubi, (Commercial)   ECOS, SDPT3, (free)
* `CVXR` [@fu2018cvxr] is a convex modeling language in R.


## References
